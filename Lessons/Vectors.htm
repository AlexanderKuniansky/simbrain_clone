<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Vectors</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<link href="../Cogs103/middle.css" rel="stylesheet" type="text/css" />
<link href="../Cogs103/navi.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <a href='../Cogs103/index.html'><div class="header"><span></span></div></a>
  <div id="navcontainer">
  <ul>
  <li> <a href="http://www.simbrain.net/Cogs103/Syllabus_COGS_103.doc">The Syllabus</a> </li>
  <li><a href="http://www.simbrain.net/Downloads/downloads_main.html">Simbrain Download</a> </li>
  <li><a href="../Lessons/Contents.htm">Lessons / Labs</a></li>
  <li><a href="http://lists.husserl.net/mailman/listinfo/cogs_103">Mailing List</a> </li>
  <li><a href="Homeworks.html">Homework</a></li>
  <li><a href="http://www.simbrain.net/index_new.html">The Simbrain Page</a> </li>
  </ul>
  <div class="body">

<p><span class="heading">Vectors and Vector-valued Functions </span></p>
<p class="heading2">List Vectors</p>
<p>Fundamental to neural network modeling is the concept of a vector. For our purposes, vectors are simply ordered lists of numbers (the actual definition is more complex, but we'll keep things informal).  These are sometimes called &quot;list vectors&quot; to distinguish them from the more formal vectors mathematicians deal with. Since neural networks deal with lots of numerical information, list vectors make it easy to describe certain aspects of network operation in a compact manner. </p>
<p>Some examples of vectors in our sense of the term are as follows:</p>
<blockquote>
  <p>(0,0)<br>
    (0,1)<br>
    (1,0)<br>
    (1,1)</p>
</blockquote>
<p>The numbers inside a vector are called &quot;<strong>components</strong>.&quot; The <strong>dimension</strong> of a vector corresponds, in this context,  how many components it has. So the vectors above are 2-dimensional vectors. Here is a list of 5-dimensional vectors:</p>
<blockquote>
  <p>(0,-1, 1, .4, 9)<br>
(-1, 2, 4, -3, 9)<br>
(0, 0, 0, -1, -1 )<br>
(0, -1, 0, -1, 0 )<br>
  </p>
</blockquote>
<p>You may also be familiar with vectors in the sense of directed line segments with arrows at the end. Vectors in our sense are just the coordinates of the arrow-end of those kind of vectors. Vectors are sometimes described by special notation and variables, such as a bold-face <strong>x</strong> or an x with a bar or arrow on top. We will not use this notation in these modules. </p>
<p class="heading2"> Vectors in Neural Networks </p>
<p>Any list of numbers can be thought of as a vector in our sense.  However, there are a few lists of numbers that are  typically treated as vectors in neural networks. In particular, lists of activation values, weight strengths, and input values are often treated as vectors. Weights are also treated as &quot;matrices,&quot; which are more like tables than lists. </p>
<blockquote>
  <p class="heading2">Activation vectors</p>
  <p>Any subset of a neural network's neurons can be described by an activation vector, each component of which corresponds to the activation of one of those neurons. </p>
  <p>An obvious activation vector is the one which describes the state of activity of an entire network. For example, we can describe this network:</p>
  <p><img src="images/Simple3.jpg" width="256" height="258"> </p>
  <p>using the vector (-1,-4, 5, 2). As the network changes values over time, this vector changes. In another module we will see how the changing activation vectors of a neural network can be visualized as a moving point in a vector-space.</p>
  <p>In addition to describing the state of all a network's neurons by an activation vector, we can describe some particular subset of its neurons using an activation vector. Often these correspond to &quot;layers.&quot; The concept of a layer has no formal definition, but the neurons in a layer are often shown lined up next to one another. In the network above there is a 3-dimensional input activation vector on the left, (-1, -4, 5), and a 1-dimensional output activation vector on the right, (2). </p>
  <p>Or consider  this network:</p>
  <p><img src="images/ThreeLayerActive.jpg" width="248" height="239"></p>
  <p>&nbsp;</p>
  <p>The bottom layer is called an &quot;input layer,&quot; the middle layer is a &quot;hidden layer,&quot; and the output layer is an &quot;output layer.&quot; Each of these is associated with an activation vector:</p>
  <blockquote>
    <p>input unit activation vector (.2, .1, .7)<br>
      hidden unit activation vector (.9, 0)<br>
      output unit activation vector (9, 0, 0)</p>
  </blockquote>
  <p>The overall activation vector for the network above is (.2, .1, .7, .9, 0, 9, 0, 0) </p>
  <p>Note that sometimes we will simply say &quot;input vector&quot; for input activation vector, though the phrase &quot;input vector&quot; has another meaning, discussed below (how the phrase is used specifically depends on the context). </p>
  <p class="heading2">Weight vectors and matrices</p>
  <p>The set of weights feeding in to a neuron is sometimes described as a weight vector. For example, in the first example above, the incoming weight vector is (-1, 1, 1) . Something similar can be done for the set of weights extending out of a neuron (an &quot;outgoing weight vector&quot;).  The set of all the weights in a neural network can also be described by a weight vector (remember, any list of numbers is a vector in our sense).</p>
  <p>A more common way of representing weights in a neural network is using a matrix, which is like a spreadsheet or a table. A matrix is a rectangular array of numbers arranged in rows and columns.  A matrix representation of a set of weights describes all the synapses connecting two sets of neurons, for example all the weights connecting an input to an output layer. If two neurons are not connected, then that component of the matrix has a 0.</p>
  <p>Remember that weight strengths are indexed by the source and target neurons they connect. This lends itself naturally to matrix representation. Given a set of <em>n </em>source neurons and <em>m </em>target neurons, we have an <em>n </em>x <em>m</em> matrix (one with <em>n</em> rows and <em>m</em> columns), with one cell for each possible connection from a source neuron to a target neuron. For example, in the example above, there are three source neurons in the input layer, and two target neurons in the hidden layer. Each of the three source neurons 1,2, and 3 can (and in that case does) connect to one of the neurons 1,2 in the hidden layer. We thus have a matrix of 3 rows and 2 columns: </p>
  <table width="78" border="0">
    <tr>
      <td width="34"><strong><em>w<sub>11</sub></em></strong></td>
      <td width="34"><strong><em>w<sub>12</sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>w<sub>21</sub></em></strong></td>
      <td><strong><em>w<sub>22</sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>w<sub>31</sub></em></strong></td>
      <td><strong><em>w<sub>32</sub></em></strong></td>
    </tr>
  </table>
  <p>Each row represents a source neuron (and can be thought of as an outbound weight vector); each column represents a target neuron (and can be thought of as an inbound weight vector). In the example above the real values are:</p>
  <table width="78" border="0">
    <tr>
      <td width="34"><strong><em>- 3.5 </em></strong></td>
      <td width="34"><strong><em>-4.3<sub></sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>-2.5<sub></sub></em></strong></td>
      <td><strong><em>5.0<sub></sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>5.0<sub></sub></em></strong></td>
      <td><strong><em>-3.5<sub></sub></em></strong></td>
    </tr>
  </table>
  <p>To verify this open up the network <span class="command">networks ) bp ) 3-2-3.xml </span>and check the weights by using tooltips or double clicking on them. Something similar can be done any time you consider a source and target population of neurons.</p>
  <p>The same analysis can be done when one considers an entire set of neurons in a network relative to itself. In that case you look at every single neuron as a possible target or source, and go through all the combinations. For example, in a 4 node network like the one above, the set of all possible synapses (assuming that there are not multiple synapses connecting a given pair of neurons) is given by a 4 x 4 matrix. </p>
  <table width="116" border="0">
    <tr>
      <td><strong><em>w<sub>11</sub></em></strong></td>
      <td><strong><em>w<sub>12</sub></em></strong></td>
      <td><strong><em>w<sub>13</sub></em></strong></td>
      <td><strong><em>w<sub>14</sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>w<sub>21</sub></em></strong></td>
      <td><strong><em>w<sub>22</sub></em></strong></td>
      <td><strong><em>w<sub>23</sub></em></strong></td>
      <td><strong><em>w<sub>24</sub></em></strong></td>
    </tr>
    <tr>
      <td width="34"><strong><em>w<sub>31</sub></em></strong></td>
      <td width="34"><strong><em>w<sub>32</sub></em></strong></td>
      <td width="34"><strong><em>w<sub>33</sub></em></strong></td>
      <td width="34"><strong><em>w<sub>34</sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>w<sub>41</sub></em></strong></td>
      <td><strong><em>w<sub>42</sub></em></strong></td>
      <td><strong><em>w<sub>43</sub></em></strong></td>
      <td><strong><em>w<sub>44</sub></em></strong></td>
    </tr>
  </table>
  <p>Again there are zeros where there are no connections. In the example above we have: </p>
  <table width="116" border="0">
    <tr>
      <td><strong><em>0</em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>-1<sub></sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>1<sub></sub></em></strong></td>
    </tr>
    <tr>
      <td width="34"><strong><em>0<sub></sub></em></strong></td>
      <td width="34"><strong><em>0<sub></sub></em></strong></td>
      <td width="34"><strong><em>0<sub></sub></em></strong></td>
      <td width="34"><strong><em>1<sub></sub></em></strong></td>
    </tr>
    <tr>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
      <td><strong><em>0<sub></sub></em></strong></td>
    </tr>
  </table>
  <p>There are only three filled components, corresponding to the three synapses. They are all part of neuron 4's incoming weight vector. The diagonal, notice, is all 0's, which means there are no self connections.</p>
  <p>Note that the whole set of synapses in the brain can be thought of in this way, as a huge 100-billion x 100-billion matrix. </p>
  <p class="heading2">Input and Output Vectors</p>
  <p>The set of input values <em>I</em><sub>i</sub> to a set of neurons is also a vector. This is almost, but not quite the same as the activation vector for an input layer (unless the activation functions of the neurons in the input layer are the identity function). An input vectors is a set of numbers, all of which will be fed to the input nodes of a network. </p>
  <p>In Simbrain, you can directly create input vectors using the Dataworld world. Dataworlds look like this: </p>
  <p><img src="images/Dataworld.png" width="657" height="144"><br>
  Each row has a possible input vector, that is, a set of values which can be fed to some set of input neurons. Pressing the send button on different rows sends different input vectors to the network. Output vectors are activation vectors corresponding to the output nodes of a neural network.</p>
</blockquote>
<p class="heading2">&nbsp;</p>
<p class="heading2">Example: Olfactory inputs</p>
    <p>What kind of vector inputs do we receive from the real world, and how can this be modeled in a neural network? Olfaction--our sense of smell--provides a nice example.</p>
    <p>Objects in the world emit chemicals which bind to receptors inside a wet bank of tissue in our noses called the nasal epithelium. Different objects produce chemicals which have a characteristic impact on our noses. If a person's nasal epithelium has 100-million sensory neurons, then olfactory inputs for that person consist of 100-million dimensional vectors, lists of 100-million numbers, corresponding to the activity of each receptor in the nose, in response to a given stimulus. </p>
    <p>Objects, then, can be associated with input vectors, or &quot;stimulus vectors,&quot; which describe their impact on a given sensory modality. If we encounter more than one object, we smell them all. Thus multiple stimulus vectors are added together. Depending on where an object is relative to a creature's sensors, the impact will be different. As we come closer to those objects, more of those chemicals are released and the same pattern will occur only stronger, and conversely as we move away from objects. Thus the total pattern of inputs to a creature's olfactor receptors is a function of the objects in its environment together with their locations relative to the creature's receptors. </p>
    <p>Simbrain provides an interface for modeling this, via &quot;Odor World.&quot; By double clicking on any object in an Odor World, the following dialog box appears:</p>
    <blockquote>
      <p><img src="images/OdorWorld_StimVector.png" width="360" height="300"></p>
    </blockquote>
    <p>The important part of this dialog is the table of &quot;Stimulus values&quot; in the middle. Each row of this table corresponds to the impact this object will have on a specific input node. The number to the left of each row is a &quot;stimulis id&quot; or &quot;receptor id,&quot; which is correlated with a receptor  of the same id in a neural ntwork. The whole set of stimulus values, then, will produce  an input vector to a neural network, a set of values that will be sent to a set of input neurons. By associating different objects with different stimulus vectors, you can make them have a differential impact on the sensory neurons of a network. For example, you can make it the case that every swiss cheese will provide an input value of 10 to receptor-1, while fish will send inputs of 10 to receptor-2.</p>
    <p>Stimulus values correspond to &quot;maximal&quot; values in the sense that they will diminish the farther away the agent is from the relevant object. The way the diminishment occurs is set in the dispersion tab. The values can change in various ways, which can be set in this the &quot;stimulus dispersion&quot; tab. </p>
    <p><img src="images/OdorWorld_StimDispersion.png" width="372" height="232"></p>
    <p>To get a sense of these functions, open the simulation <span class="command">lessons &gt; local_dist.xml</span> and play with the settings in the entity dialogs. </p>
    <p>Finally, note that how the stimulus value is changed into an actual input value depends on the location of the sensory apparatus on a creature. I hear things to my left more in my left ear than my right, because my left ear is closer to the source of the sound. The location of a receptor can be set in Simbrain in the agent tab of the dialog that opens when you double click on a mouse:</p>
    <p><img src="images/OdorWorld_Agent.png" width="385" height="194"></p>
    <p>&nbsp;</p>
    <p>Although Odor Worlds are built around olfaction, nothing prevents us from interpreting these functions as providing a general way of producing input vectors. After all, something similar occurs with other sensory modalities. Different objects will produce characteristic patterns of activity--input vectors--on the eyes, ears, and skin. Thus OdorWorld can be seen as a general framework for modeling the differential impact of objects on an agent's sensory apparatus.</p>
    <p>There is much more to say about input vectors: in many cases they will only represent real stimuli in a very rough way. And they have different characteristics, some corresponding to &quot;local,&quot; some to &quot;distributed&quot; representations. More on these distinctions in later modules. </p>
    <p class="heading2">Example 3: Motor outputs</p>
    <p> We have seen how sensory stimulations take the form of input vectors; how do output vectors get turned in to behavior in the world? In the brain of course, this is a complex process, involving millions of signals sent to a set of muscles. In fact, to describe the &quot;command&quot; that makes a single muscle do something we would have to count the number of neurons attaching to a muscle (millions no doubt). Then a given vector of values to that muscle will make it contract in a specific way. </p>
    <p>A simple form of the process can be modeled in odor world. Output neurons send their values to specific commands, e.g. &quot;go straight&quot; and &quot;turn right.&quot; How far straight or right the creature goes depends on the activation value and the <span class="command">straight factor</span> and <span class="command">turn factor</span> set in the agent tab of the agent dialog, pictured above. </p>
    <p>For example, a network with three output nodes would produce three-dimensional output vectors which would be interpret red in the following way: </p>
    <blockquote>
      <p>(Left, Right, Straight)</p>
    </blockquote>
    <p>The value of each number determines how much it will go left, right, or straight respectively (again, these numbers are also  weighted by quantities called &quot;turn factor&quot; for turning and '&quot;straight factor&quot; for forward movement,  set in the agent tab). For example: </p>
    <blockquote>
      <p> (0,0,1) Goes forward<br>
(0,0,0) Stays Still <br>
(1,0,0) Turns left <br>
(1,5,0) Turns right (the rightward motion outweighs the leftward) <br>
(1,5,1) Turns right and goes forward</p>
</blockquote>
    <p>In another module on sensori-motor coordination we discuss how input vectors are coordinate with output vectors to make an agent behave sensibly in the world. </p>
    <p class="heading2">Vector-valued Functions </p>
    <p>We have seen how inputs to and outputs from a system are treated as high-dimensional vectors in neural networks. What about the processing in the middle? Of course, this is where most of the action happens in neural network theory--inside the neurons and synapses. </p>
    <p>One way of looking at the internal processing of a real or artificial neural network is as computing a series of vector to vector transformations based on the intervening synapses, which can be treated as weight matrices. An input vector is turned into an activation vector in the visual processor, which is turned into a vector in the categorization module, and so forth. Somewhere along the line an output vector is produced as well. Thus the brain (and mind) can be thought of as a big series of vector to vector transformations mediated in part by weight matrices.</p>
    <p>Let us begin by considering a single step in such a process, a single vector to vector transformation. Such a transform can be thought of as a vector valued function. A function is sometimes described as a rule which you can use to associate one thing with another. In another module we talked about activation functions, which are rules for associating the net input to a neuron with the activation of that neuron.</p>
    <p>However, a whole network can also be thought of as computing a function. For example, consider this network again:</p>
    <blockquote>
      <p><img src="images/ThreeLayer.jpg" width="208" height="244"> </p>
    </blockquote>
    <p>This network well associate input activation vectors with output activation vectors. In can be thought of as a rule which associates 3-dimensional vectors with 3-dimensional vectors. We can also think of it as a pair of functions, one composed with the other, where one function takes 3-dimensional input vectors to 2-dimensional hidden layer vectors, and the other function takes 2 dimensional hidden layer vectors to 3 dimensional output layer vectors. </p>
    <p>The weights of this network can be describes in terms of two matrices, a 3 x 2 matrix of input to hidden layer weights, and a 2 x 3 matrix of hidden to output layer weights. As these weights change so too does the vector valued function the network as a whole computes. Learning can therefore be thought of as a way of changing vector to vector functions, or transformations, by modifying weight matrices. In fact the brain as a whole can be thought of as computing a whole series of vector to vector transformations, mediated by weight matrices which are slowly updated to optimize those transformations. </p>
    <p>In later modules we will consider automatic procedures for updating weights. Let us first consider some very simple vector valued functions. </p>
    <p><span class="heading2">Example 1: Logic Gates </span></p>
    <p>A very simple form of vector function to compute with a neural network is one which implements a logic gate. A logic gate can be thought of as a function---sometimes called a &quot;boolean function&quot;---which associates truth values (true and false, T and F) with other truth values. These functions are at the basis of all computing machines in electronic form they are literally a kind of gate. Functions can be represented by tables, where the first columns represent the inputs and the last columns represent the outputs. Here are some classic boolean functions in table form. The first two componets are the inputs, and the last three are three different outputs (hence we have three functions shown here, one each for the last three columns): </p>
    <div align="center">
      <table width="374" border="0" cellpadding="5">
        <tr class="heading">
          <td width="18">P</td>
          <td width="26">Q</td>
          <td width="85">P and Q </td>
          <td width="92">P or Q </td>
          <td width="91">P xor Q </td>
        </tr>
        <tr class="heading">
          <td>F</td>
          <td>F</td>
          <td>F</td>
          <td>F</td>
          <td>F</td>
        </tr>
        <tr class="heading">
          <td>F</td>
          <td>T</td>
          <td>F</td>
          <td>T</td>
          <td>T</td>
        </tr>
        <tr class="heading">
          <td>T</td>
          <td>F</td>
          <td>F</td>
          <td>T</td>
          <td>T</td>
        </tr>
        <tr class="heading">
          <td>T</td>
          <td>T</td>
          <td>T</td>
          <td>T</td>
          <td>F</td>
        </tr>
      </table>
    </div>
    <p align="center">&nbsp;</p>
    <p>These logic functions are easy to understand if you think about them in terms of examples. For example, if P means &quot;I ate cheese&quot; and Q means &quot;I ate eggs,&quot; then P and Q means &quot;I ate cheese and I ate eggs&quot; or more simply &quot;I ate eggs and cheese.&quot; If both P and Q are false, if I didn't eat eggs and I didn't eat cheese, then I surely didn't eat eggs and cheese. Similarly if either of P and Q is false: if I didn't eat eggs, for example, then I didn't eat eggs and cheese. But if both P and Q are true, then it's true that I ate both cheese and eggs. Try the same for Or and it should make sense. &quot;xor&quot; is the only unusual function: it is like a strict or, which is false when both p and q are true. </p>
    <p>If we interpret truth as 1 and falsity as 0, we can represent these logic gates in numerical form:</p>
    <p align="center">&nbsp;</p>
    <div align="center">
      <table width="368" border="0" cellpadding="5">
        <tr class="heading">
          <td width="21">P</td>
          <td width="24">Q</td>
          <td width="82">P and Q </td>
          <td width="73">P or Q </td>
          <td width="106">P xor Q </td>
        </tr>
        <tr class="heading">
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr class="heading">
          <td>0</td>
          <td>1</td>
          <td>0</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr class="heading">
          <td>1</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr class="heading">
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>0</td>
        </tr>
      </table>
    </div>
    <p>Let's focus on the and gate:</p>
    <p align="center">&nbsp;</p>
    <div align="center">
      <table width="170" border="0" cellpadding="5">
        <tr class="heading">
          <td width="25">P</td>
          <td width="27">Q</td>
          <td width="80">P and Q </td>
        </tr>
        <tr class="heading">
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr class="heading">
          <td>0</td>
          <td>1</td>
          <td>0</td>
        </tr>
        <tr class="heading">
          <td>1</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr class="heading">
          <td>1</td>
          <td>1</td>
          <td>1</td>
        </tr>
      </table>
    </div>
    <p align="center">&nbsp;</p>
    <p>This can be thought of as a vector valued function from 2-d vectors to 1-d vectors. And we have already seen how these kinds of functions can be implemented in a neural network. This function is relatively easy to implement, simply by requiring that the net input to the output, &quot;P and Q&quot; neuron is a binary neuron with a threshold which is only exceeded when both inputs to it are 1 (say, threshold 1.5, with weights of 1 and 1). </p>
    <p>Try opening the different networks in the directory logic gates in Simbrain to see how they work. You will notice that XOR requires an extra layer of neurons to solve. This will be important later. </p>
    <p>Of course vector functions are much more complex than this in practice; but logic gates are a nice simple place to start. </p>
    <p class="heading2">Vector-based representations and the Philosophy of connectionism </p>
    <p>The ideas presented in this module form the basis for one general philosophical view of the operation of mind and brain, which originates largely in work of Paul and Patricia Churchland:</p>
    <p><img src="images/Churchlands.gif" width="274" height="238"> </p>
    <p>According to this view, the mind / brain is a huge vector processor. The inputs are vectors, the outputs are vectors, and all the stuff in the middle consists in vector-to-vector transformations. Thus the mind can be modeled as a complex numerical processor, involving millions of differentially active neurons transforming themselves along complex synaptic channels. Mathematically, these are vector-to-vector transformations mediated by weight matrices. Our knowledge of the world is embedded in all these synaptic connections and our competent performance in the world consists in our using that knowledge to successfully transform input vectors into output vectors. </p>
    <p>This view is in contrast to more traditional views according to which mental representations are like statements in a language. We will see more about this in a later module, and already saw that the division between classical AI and connectionism was an important one in the history of cognitive science. </p>
    <p class="heading2">Study Questions</p>
    <p>Write down three eight-dimensional vectors.</p>
    <p>If we interpret output vectors as producing movement of the form (move left, go straight, move right), what will a networks with the following output vectors do:</p>
    <blockquote>
      <p>(0,0,0)<br>
        (1,0,0)<br>
        (0,1,1)<br>
        (
        -1, 5, 1)</p>
    </blockquote>
    <p><br>
      Write the activation vector and weight vector for a neural network with these properties:</p>
    <blockquote>
      <p>w14 = -1<br>
      w24=7<br>
      w34=1<br>
      a1 = -9<br>
      a2 = 1<br>
      a3 = 10</p>
    </blockquote>
    <p>Draw a neural network which could implement a function which associates 5 dimensional vectors with 2 dimensional vectors.</p>
    <p>Write down the vector valued function corresponding to exclusive or.</p>
    <p>&nbsp; </p>
</body>
</html>
