<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Hebbian Learning</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<link href="../Cogs103/middle.css" rel="stylesheet" type="text/css" />
<link href="../Cogs103/navi.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <a href='../Cogs103/index.html'><div class="header"><span></span></div></a>
  <div id="navcontainer">
  <ul>
  <li> <a href="http://www.simbrain.net/Cogs103/Syllabus_COGS_103.doc">The Syllabus</a> </li>
  <li><a href="http://www.simbrain.net/Downloads/downloads_main.html">Simbrain Download</a> </li>
  <li><a href="../Lessons/Contents.htm">Lessons / Labs</a></li>
  <li><a href="http://lists.husserl.net/mailman/listinfo/cogs_103">Mailing List</a> </li>
  <li><a href="Homeworks.html">Homework</a></li>
  <li><a href="http://www.simbrain.net/index_new.html">The Simbrain Page</a> </li>
  </ul>
  <div class="body">

<p class="heading">Hebbian Learning</p>
<p>Hebbian learning is perhaps the oldest and simplest learning algorithm for neural networks. It is biologically plausible, but has limitations which prevent it from being widely used.</p>
<p>The basic idea of Hebbian learning is that neurons which are connected, and which are both active at the same time, will strengthen their connection. As it is sometimes said, &quot;neurons which fire together, wire together.&quot; The original formulation of this rule is due to Donald Hebb, who proposed the idea in the 1940's, well before there was any experimental support for the idea. As he put it:</p>
<blockquote>
  <p> When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A&rsquo;s efficiency, as one of the cells firing B, is increased (62).  </p>
</blockquote>
<p>Today it is known that something like Hebbian learning occurs in the brain, called &quot;Long Term Potentiation,&quot; though how precisely this works is still being debated. </p>
<p>Psychologically, this was thought to correspond roughly to building up association between ideas. If I always hear bells when I see meat, then those ideas are associated in my mind. Hebbian learning proposes one idea about how this happens in the brain. In fact there are even older proposals of roughly this sort (in Bain, and even in the philosopher Descartes) , according to which the association of ideas corresponds to strengthening of some sort of connection in the brain.</p>
<p>We will discuss pattern assocation and Hebbian learning in other modules, as well as variants of Hebbian learning. In this module we briefly describe Hebbian learning itself.</p>
<p class="heading2">Hebb Rule</p>
<p>For the Hebb rule, the change in a weight is equal to the product of a learning rate epsilon, the source activation, and the target activation. The learning rate epsilon controls how much the weight changes on each iteration. </p>
<p><img src="images/Hebbian1.png" width="150" height="45"></p>
<p>Note that this is an unsupervised learning rule: the weight changes based only on local information about the two nodes it connects. </p>
<p class="heading2">Example 1 </p>
<p>For example, consider this network:</p>
<p><img src="images/Simple3.jpg" width="256" height="258"></p>
<p>Suppose all the activations are clamped and the weights are:</p>
<blockquote>
  <p>w<sub>1,4</sub> = -1<br>
    w<sub>2,4</sub> = 1 <br>
    w<sub>3,4</sub> = 1 </p>
</blockquote>
<p>Now suppose epsilon is set to 1. What will the new values of the weights be on each time step? Remember,</p>
<blockquote>
  <p>w(t+1) = w(t) + delta-w</p>
</blockquote>
<p>In this case:</p>
<blockquote>
  <p>w<sub>i,j</sub>(t+1) = w<sub>i,j</sub>(t) + epsilon * a <sub>i</sub> * a <sub>i</sub></p>
</blockquote>
<p>So;</p>
<blockquote>
  <p>w<sub>1,4</sub> = -1 + 1 * -1 * 2 = -3 <br>
    w<sub>2,4</sub> = 1 + 1 * -4 * 2 = -7 <br>
    w<sub>3,4</sub> = 1 + 1 * 5 * 2 = 11 </p>
</blockquote>
<p>For the next time step, we will have:</p>
<blockquote>
  <p>w<sub>1,4</sub> = -3 + 1 * -1 * 2 = -5<br>
    w<sub>2,4</sub> = -7 + 1 * -4 * 2 = -15<br>
    w<sub>3,4</sub> = 11 + 1 * 5 * 2 = 21 </p>
</blockquote>
<p>What has this network learned? Well, if we change neuron 4 from being clamped to a linear activation function with slope 1, and we use the network now, with these new weight values, the net input to neuron 4 will be (-5 * -1) + (-15 * -4) + (21 * 5) = 5 + 60 + 105 = 170! Since neuron 4 uses a linear activation function with slope 1, its activation will also be 170. So the activation is greatly increased by these changes. You can see that without clipping these weights will just head towards extreme values. This is the problem with Hebbian learning, it tends to push weights towards extreme values.</p>
<p>One way to slow this down is to use a smaller value for epsilon. Suppose in the example above epsilon was set to .01. Now what would the new weight values be at each time step?</p>
<p class="heading2">Example 2: Mixing training and performance </p>
<p>Previously we distinguished training from performance. Above we just trained the weights using the Hebb rule, and clamped the neurons. But what if we allow the neurons to change at the same time as the weights? This is obviously more complex. To try this out, let us return to the example above, and leave the first three neurons clamped, but change the output node so that it uses the linear activation function with slope 1. </p>
<blockquote>
  <p>w<sub>1,4</sub> = -1 + 1 * -1 * 2 = -3 <br>
    w<sub>2,4</sub> = 1 + 1 * -4 * 2 = -7 <br>
    w<sub>3,4</sub> = 1 + 1 * 5 * 2 = 11 </p>
</blockquote>
<p></p>
<p>Now the activation of neuron 4 will be  (-1 * -3) + (-4 * -7) + (5 * 11) = 3 + 28 + 55 = 86.  So the weight changes at the next time step are:</p>
<blockquote>
  <p>w<sub>1,4</sub> = -3 + 1 * -1 * 86 = -89<br>
    w<sub>2,4</sub> = -7 + 1 * -4 * 86 = -351<br>
  w<sub>3,4</sub> = 11 + 1 * 5 * 86 = 441</p>
</blockquote>
<p></p>
<p>And the activation of neuron 4 will now be (-1 * -89) + (-4 * -351) + (5 * 441) = 89 + 1404 + 2205 = 3698! </p>
<p>Wow! You can see that the changes are really amplified now. And again, we see that without some measures limiting the growth of weights and neurons, Hebbian learning is not useful. </p>
<p class="heading2">Other Examples</p>
<p>Can Hebbian learning do anything useful? In some cases yes. We will see in another module how Hebbian networks can perform pattern classification tasks. </p>
<p>There are also other forms of hebbian learning which do things to control the explosion of weight growth or decay. Some variants we look at are competitive networks and  Hopfield networks. </p>
</body>
</html>
