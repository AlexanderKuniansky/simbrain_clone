<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Basic Function</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<style type="text/css">
<!--
.style1 {font-weight: bold}
.style2 {
	font-weight: bold;
	font-style: italic;
}
-->
</style>
</head>

<body>
<blockquote>
  <p><span class="heading">Activation Functions</span></p>
  <p>In other modules we have seen in a qualitative way the way neural networks channel information. Artificial neurons or &quot;nodes&quot; have activation values that, on analogy with electrical signals in the brain, propagate through networks of neurons along weighted connections. In this module we begin to review the basic mathematics used to simulate this kind of propagation.</p>
  <p class="heading2">Basic Quantities </p>
  <p>First, let us associate symbols with the fundamental quantities used in neural network modeling. We will represent the activation of a neuron <em>i </em>by the letter <strong><em>a</em></strong>, the strength of a weight connecting neuron <em>i</em> to neuron <em>j</em> <strong><em>w</em><sub>ij</sub></strong> , and external inputs from the environment by <strong><em>I</em></strong>. We also introduce a new quantity, &quot;bias,&quot; <strong><em>b </em></strong>which corresponds to a fixed amount of input to a neuron. It can also be thought of as a &quot;base-line&quot; level of activity of a neuron. It is like an external input except that it typically does not change. There will be many more symbols later, but this is a good start. To review: </p>
  <blockquote>
    <p><span class="style1"><em>a</em><sub>i</sub></span>: activation of neuron <em>i</em>. </p>
    <p><strong><em>w<sub>ij</sub></em></strong>: strength of connection from neuron <em>i</em> to neuron <em>j</em>. </p>
    <p><span class="style2">I</span>: external inputs to a neuron.</p>
    <p><em><strong>b</strong></em>: bias term of a neuron.</p>
  </blockquote>
  <p><em>Terminological Notes</em>:  relative to a weight <strong><em>w</em><sub>ij</sub></strong> neuron <em>i</em> is a pre-synaptic &quot;source&quot; or &quot;input&quot; neuron, while neuron <em>j</em> is a post-synaptic &quot;target&quot; or &quot;output&quot; neuron. Also note that input and output in this context do not necessarily mean couplings to an external environment, but rather other connected neurons. <em>I</em> above explicitly means an input from a source external to the network. Finally, note that though <em>b</em> and <em>I </em>are not given subscripts above, they can be, since they are associated with specific neurons.</p>
  <p class="heading2">Weighted Inputs </p>
  <p>We can now introduce our first, and perhaps most fundamental derived quantity, the &quot;weighted inputs&quot; or &quot;net input&quot; to a neuron, which corresponds to a weighted sum of all the signals coming in to a neuron from other neurons, plus any bias and external inputs from the environment. We symbolize net input to a neuron <em>j </em>by <em><strong>net<sub>j</sub></strong></em> (in the Simbrain documentation it is symbolized <strong><em>W</em></strong><sub>j</sub>) </p>
  <blockquote>
    <p><span class="style2">net<sub>j</sub></span>: net input to neuron <em>j</em>.</p>
  </blockquote>
  <p>The net input to a neuron <em>j </em>is computed by multiplying the activations of all of the <em>n </em>source neurons <em>a<sub>i</sub></em> connected to it by the intervening weights, and adding any external input <em>I</em> and bias <em>b</em>: </p>
  <blockquote>
    <p><img src="images/net_input.jpg" width="242" height="82"></p>
  </blockquote>
  <p>It's not too hard to see how this captures the basics of signal propagation in a network. If a neuron is very active, and it is connected by positive, excitatory weights to another neuron, then it will pass that high level activation along to the next neuron. How much of it passes along depends on how strong the weight. If neuron is very active, but is connected by negative, inhibatory weights, then it will &quot;cool down&quot; the neurons it is connected to. The net input to a neuron represents the sum of all these different influences from other neurons. </p>
  <p>Let's consider some examples:</p>
  <p>&nbsp;</p>
  <blockquote>&nbsp;</blockquote>
  </blockquote>
<p class="heading2">Activation functions</p>
<p>Now that we've defined all these quantities, we are in a position to define some basic &quot;activation functions.&quot; An activation function is a method or rule for updating the activation <strong><em>a </em></strong>of neuron or node over time. For the most part, activation functions are actual functions <em>f</em>(x)<em>, </em>which take the net input <span class="style2">net<sub>j</sub></span> to neuron <em>j</em> as  input and produce an activation value <span class="style1"><em>a</em><sub>j</sub></span> as output, though in some cases they don't take any inputs at all. For example, a random neuron simply takes a different value at every time step.</p>
<p>To get a sense of the breadth of activation functions available, double click on a neuron in Simbrain and note all the choices of neuron type in the drop down box. Or, go to the Simbrain documentation and scan through all the neuron types.</p>
<p>We will cover the three most common forms of activation function in this section: binary, linear, and sigmoidal. </p>
<p class="heading2">Binary Activation Function </p>
<p>We begin with a very basic activation function, which goes back to one of the earliest discussions of neural networks in McCulloch and Pitts. </p>
<p>The value of a binary unit is an upper value <em>u</em> if net input is greater than a threshold value, and a lower value <em>l</em> otherwise:</p>
<blockquote>
  <p><img src="images/binary_function.jpg" width="223" height="79"></p>
</blockquote>
<p>The upper and lower bounds of a binary neuron are set in the upper and lower bound fields, respectively, in Simbrain. </p>
<p>Since these neurons can take on only one of two values, an upper and a lower value, they are called &quot;binary&quot; units. Whether they take on the upper or lower value depends on whether net input is greater than or less than a threshold value &theta;. Thus they are also called &quot;threshold units&quot; or &quot;linear threshold units&quot; (the &quot;linear&quot; refers to net input, which is a linear sum). </p>
<p>Binary Function</p>
<blockquote>
  <p><img src="images/BinaryGraph.gif" width="161" height="110"></p>
  <p>&nbsp;</p>
  <p>&nbsp;</p>
</blockquote>
<p class="heading2">Linear Activation Function </p>
<p>Linear activation functsion compute activation as a linear function of net input. Thus netinput is either scaled up or down by a factor <em>m </em>corresponding to the slope of the linear function: </p>
<p><img src="images/linear_function.jpg" width="158" height="67"></p>
<p>Often we want to impose limits on how big or small activation is, in which case we use &quot;clipping.&quot; In that case we have what is called a &quot;piecewise linear&quot; function. Here is a graph of a piecewise linear function with slope 1.2, upper bound 1, and lower bound 0. </p>
<blockquote>
  <p><img src="images/LinearGraph.gif" width="164" height="104"></p>
</blockquote>
<p>&nbsp;</p>
<p>Clipping can be turned on or off in Simbrain. If it is on, the upper and lower bounds set the bounds of function. Slope and bias can also be set in the neuron dialog box.</p>
<p>In the case where slope = 1 and clipping is turned off, the neuron's activation function is the identity function, so that the activation of a neuron just is its net input.</p>
<blockquote>
  <p>&nbsp; </p>
</blockquote>
<p class="heading2">Sigmoidal</p>
<p>Sometimes it is useful to have an activation function whose curve is smoothe. </p>
<p>For this function I will not include the equation, since what matters is more the form of the graph. In Simbrain you can set the upper aymtote (1 below), lower asymptote (0 below), the inflection point (.5 below), and also how &quot;steep&quot; the curve is (steepness). </p>
<p>&nbsp; </p>
<blockquote>
  <p><img src="images/SigmoidalGraph.gif" width="161" height="107"></p>
</blockquote>
<p>&nbsp;</p>
<p class="heading2">Cognitive Modeling Revisisted</p>
<p>In another module we saw that activation corresponds to a general level of activity of neurons or nodes, and that strength corresponds to how well a weight is able to pass information along. It is worth noting that positive, negative, and zero values have specific meaning, which corresponds to specific properties of neurons and is reflected in Simbrain. </p>
<blockquote>
  <p><strong>Excitatory synapse</strong>: a weight with a positive value is excitatory. An excitatory synapse (greater than 1) <em>amplifies</em> whatever signal it receives. Positive inputs are made more positive, negative inputs are made more negative. </p>
  <blockquote>
    <p><em>What this represents</em><em> biologically </em>: different synapses release different neurotransmitters. Some of them cause the voltage post.</p>
    <p><em>Simbrain</em>: this corresponds, by default, to the color red.</p>
  </blockquote>
  <p><strong>Excitatory synapse</strong>: a weight with a positive value is excitatory. Pre-synaptic activation is enhanced by an excitatory synapse. </p>
  <blockquote>
    <p><em>What this represents biologically </em>: synapses which release.... </p>
    <p><em>Simbrain</em>: this corresponds, by default, to the color blue. </p>
  </blockquote>
  <p><strong>Zero synapse</strong>: a weight with a value of 0 is a way of mathematically representing the <em>absence of</em> a connection. In Simbrain this is currently represented as blue, inhibitory. </p>
  <p><strong>Excitatory synapse</strong>: a weight with a positive value is excitatory. Pre-synaptic activation is enhanced by an excitatory synapse. </p>
  <blockquote>
    <p><em>What this represents</em>: different synapses release different neurotransmitters. Some of them cause the voltage post </p>
    <p><em>Simbrain</em>: this corresponds, by default, to the color red.</p>
  </blockquote>
  <p><strong>Excitatory synapse</strong>: a weight with a positive value is excitatory. Pre-synaptic activation is enhanced by an excitatory synapse. </p>
  <blockquote>
    <p><em>What this represents</em>: different synapses release different neurotransmitters. Some of them cause the voltage post </p>
    <p><em>Simbrain</em>: this corresponds, by default, to the color red.</p>
  </blockquote>
  <p><strong>Zero neuron </strong>: a neuron with a value of 0 is a way of mathematically representing a <em>non-</em>active neuron, which is not firing, or is firing at a very low level. In Simbrain this is represented by the color white. It is sometimes useful to note that in a network of many non-firing, zero neurons, how a few are firing. In Simbrain this is seen as a wave of color through mostly white neurons.</p>
  <p><strong>Bias</strong>: the bias of a neuron can be thought as its base-line value, and indeed in the brain the base-line firing rates of differeent neurons varies.</p>
  <p><strong>Clipping / Upper and Lower Bounds</strong>: There are limits on how active or inactive a neuron can be. </p>
</blockquote>
<p><br>
</p>
<p>&nbsp;</p>
</body>
</html>
