<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Pattern Assocation</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<link href="../Cogs103/middle.css" rel="stylesheet" type="text/css" />
<link href="../Cogs103/navi.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <a href='../Cogs103/index.html'><div class="header"><span></span></div></a>
  <div id="navcontainer">
  <ul>
  <li> <a href="http://www.simbrain.net/Cogs103/Syllabus_COGS_103.doc">The Syllabus</a> </li>
  <li><a href="http://www.simbrain.net/Downloads/downloads_main.html">Simbrain Download</a> </li>
  <li><a href="../Lessons/Contents.htm">Lessons / Labs</a></li>
  <li><a href="http://lists.husserl.net/mailman/listinfo/cogs_103">Mailing List</a> </li>
  <li><a href="Homeworks.html">Homework</a></li>
  <li><a href="http://www.simbrain.net/index_new.html">The Simbrain Page</a> </li>
  </ul>
  <div class="body">

<p class="heading">Pattern Association </p>
<p>One typical application of neural networks is to problems in pattern association. A pattern association network is one which associates an input pattern with an output pattern. That is, such a network takes an input vector as input, and produces and output vector as output. Of course,  this is just another way of talking about vector valued functions. The domain of such a function is one pattern, and the range is the other pattern. For example, or gates associate inputs patterns (0,0), (0,1), etc. with output patterns (0), (1), etc. They are a kind of pattern associator.</p>
<p>Psychologically, this correesponds to a whole variety of associative phenomena, e.g. those studied under the rubric of conditoning. Pavlov's dogs learned to associate the ringing of a bell with the smell of meat. You may associate pencils and blue books with the joy or dread you feel in relation to school.</p>
<p>The importance of pattern associators is that many of the early examples of learning algorithms were designed to teach neural networks to produce different kinds of associations. In subsequent modules we will describe a variety of learning rules for pattern assocation. In this module we introduce some basic vocabulary and a method for hand crafting a specific type of pattern associator. </p>
<p>You can think of these in terms of a set of pairs of source and target vectors. </p>
<p class="heading2"> Auto-associative vs. Hetero-associative</p>
<p>One way of classifying associative networks is into auto- and hetero-associative networks. Auto-associative networks associate vectors with themselves (that is, auto-associative vectors try to produce an identity function for vectors). Hetero-associative networks associate one pattern with a different one.</p>
<p>Give examples of source / target pairs.  </p>
<p> An auto-associative pattern associator associates a pattern with itself. Why do this? We see below that neural network pattern associators have certain desierable properties that are useful for auto-associators. For example, an auto-associative network can take part of a vector and associate it with the whole vector (of course this isn't quite auto-association, strictly; but this approximation of auto-association is more what theorists have in mind). </p>
<p>Let us also recall  the distinction between recurrent and feed forward nets. E.g. compare:</p>
<table width="200" border="0" cellspacing="10" cellpadding="10">
  <tr>
    <td><img src="images/simple_net.jpg" width="113" height="156"></td>
    <td><img src="images/recurrent_network.png" width="211" height="166"></td>
  </tr>
</table>
<p>The network on the left is a feed-forward network network. It can be thought of as associating two dimensional vectors with themselves. If the source and target patterns are identical, then it is auto-associative. Otherwise it is hetero-associative.</p>
<p>The network on the right is a recurrnet network. In this case it is less obvious where the source and target patterns go. In fact, the source pattern is the pattern we supply to the network, while the target pattern is the pattern it settles into (the nearest attractor). Thus, this associates four-dimensional patterns with four dimensional patterns. Recurrent networks are almost always trained to perform auto-association.</p>
<p><br>
  <span class="heading2">Pattern Classifiers</span></p>
<p>A special kind of pattern associatior is a &quot;pattern classifier.&quot; The simplest pattern classifiers classify each input vector into one of two classes. That is, each input pattern either does or does not belong to a given class. These networks have a single output unit, and as many input units as we desire. We then specify that an input vector is &quot;in&quot; the relevant class if the output is 1, and it is not in the class if the output is 0 (or sometimes -1). We can imagine, for example, a classifier detector which says whether a pixel pattern is a circle or not, or whether it is the letter J. Or we can imagine a network which classifies milk samples as spoiled or not. The &quot;and&quot; gates correspond to those inputs 1 and 2 for which &quot;1 and 2&quot; is true. </p>
<p>We can easily generalize from single class classifiers to multi-class classifiers, simply by adding additional output nodes. Thus we could build a classifier which says which letter different pixel patterns correspond to.</p>
<p>For now will focus on single class classifiers, and in particular on classifiers which take two-dimensional vectors as inputs. Thus, we focus on networks with two input units, and one binary output unit (they can also have one or more hidden layers of units). </p>
<p class="heading2">Decision boundaries</p>
<p>We now introduce a graphical way of studying pattern classifiers with 2 input nodes. Since each input pattern is either classified as being in the class or not (e.g. as being good or bad, spoiled or not, etc.), we can graph such a classifier as follows:</p>
<p>INSERT PICTURE.</p>
<p>The position in the plane corresponds to the input vector, &quot;0&quot; corresponds to not being in the class, &quot;1&quot; corresponds to being in the class.</p>
<p>Of course we could also represent this as a vector-valued function:</p>
<p>INSERT GRAPH.  </p>
<p>The problem of finding an appropriate classification is that of drawing a line or curve around the 0's and 1's. This is called a &quot;decision boundary.&quot;</p>
<p class="heading2">Linearly separable functions</p>
<p>We will focus on a special case in which the decision boundary is a single line (not a curve). These are called &quot;linearly separable&quot; functions. Which of the following are linearly separable problems?</p>
<p class="heading2">Finding the Decision Line Manually </p>
<p>Now, with two layer networks where the output unit is a linear unit, the decision curve is a line. Thus, these networks can only solve linearly separable problems. In this case we can find the decision line manually.</p>
<p class="heading2">Generalization</p>
<p>These networks will tend to generalize. In fact you can visuzlie this in the examples above. </p>
<p>(PDP, p. 30) </p>
<p>&nbsp;</p>
</body>
</html>
