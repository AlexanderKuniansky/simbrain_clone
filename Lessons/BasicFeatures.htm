<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Basic Features</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<link href="../Cogs103/middle.css" rel="stylesheet" type="text/css" />
<link href="../Cogs103/navi.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <a href='../Cogs103/index.html'><div class="header"><span></span></div></a>
  <div id="navcontainer">
  <ul>
  <li> <a href="http://www.simbrain.net/Cogs103/Syllabus_COGS_103.doc">The Syllabus</a> </li>
  <li><a href="http://www.simbrain.net/Downloads/downloads_main.html">Simbrain Download</a> </li>
  <li><a href="../Lessons/Contents.htm">Lessons / Labs</a></li>
  <li><a href="http://lists.husserl.net/mailman/listinfo/cogs_103">Mailing List</a> </li>
  <li><a href="Homeworks.html">Homework</a></li>
  <li><a href="http://www.simbrain.net/index_new.html">The Simbrain Page</a> </li>
  </ul>
  <div class="body">
<p><span class="heading">Basic Features of Neural Networks </span></p>
<p>The opening chapter of Rumelhart and McClelland's (1988) books (the books that revived interest in neural network research) is entitlted &quot;The Appeal of Parallel Distributed Processing.&quot; Most the aspects of neural networks highlighted in that chapter are still relevant 20 years later, and indeed many introductory texts in neural networks begin with a discussion of these aspects. Let us go through the major features. </p>
<p>In doing so we are returning to a historical topic mentioned in another module. Soon after the MIT conference, a split developed between those who thought that cognition should be studied in terms of how the brain works, and those who thought that the brain was less important than the high level formal structure of cognition. In the first camp the structure of the brain is relevant to thinking, in the second camp the brain is a low level hardware medium, which does not matter to the high level processes. The split continue to this day: the role of neuroscience in cognitive science remains a controversial topic. </p>
<p>In a way, the split is about what the best metaphor for the information processing in the mind is: is it the metaphor of the brain, or of a classical computer, with RAM and a central processor? </p>
<p>In this module one thing we will be covering differences between the two approaches. </p>
<p class="heading2">Neural Networks Operate in Parallel</p>
<p>Whereas regular digital computers do things one at a time, neural networks do a lot of things at the same time. Digital computers perform computations in <em>serial</em>, while neural networks perform computations in <em>parallel</em>.To see the difference, consider a simple problem: finding which of ten cups has a bean under it. A serial approach would lift each cup up, one at a time, until the bean was found. A parallel approach would lift all ten cups up at once.</p>
<p>Why not do everything in parallel? It's clearly faster. Well, digital computers are engineered to be reliable, they always work correctly. And part of that reliability comes from the fact that they do things carefully, one at a time. So serial processing is useful from an engineering standpoint (even the latest parallel processing computers are still fundamentally serial processors, they just distribute certain functions across those serial processors). Massively parallel processors like the brain are somewhat messy, they make mistakes. And even if serial processing is slow, electricity is fast: the computer I'm writing this on performs several billion operations per second. With numbers like those we can tolerate the serial bottleneck.</p>
<p>However, the brain is not so fast. Neurons fire at most 200 times per second. They are orders of magnitude slower then the transistors on this computer. In order to solve problems in a reasonable time frame the brain operates in parallel: every neuron is always doing its own thing. The trade-off is in accuracy: the brain is a kind of messy computer, which doesn't always do the same thing twice. But given the wet, biological stuff we're made of, it works well enough.</p>
<p>Things are not quite so clean in practice. First, even though neural networks are parallel, we run them on our serial compters. I'm guessing that if you run Simbrain, you are running it on a serial computer. That is, we<em> simulate parallel processing on serial computers</em>. But that doesn't matter--these are models after all; simulations aimed at studying how the real thing works. </p>
<p>Second, even though neural networks operate in parallel, many aspects of thinking of thinking are serial. As Rummelhart and McClelland point out:</p>
<blockquote>
  <p>The process of human cognition, examined on a time scale of seconds and minutes, has a distinctly sequential character to it. Ideas come, seem promising, and then are rejected; leads in the solution to a problem are taken up, then abandoned and replaced with new ideas... Clearly any useful description of the overall organization of this sequential flow of thought will necessarily describe a sequence of states (p. 12). </p>
</blockquote>
<p>But neural networks model the &quot;microstructure&quot; of cognition, what is sometimes called the &quot;sub-symbolic&quot; level of mental processing. Even if it is true that the overall behavior of our brain--patterns of firing across millions of neurons--flows from macro-pattern to macro-pattern sequentially, these high level processes are somehow grounded in low level processes involving thousands or millions of parallel computations. </p>
<p class="heading2">Neural Networks Gracefully Degrade </p>
<p>Classical, digital computers are &quot;brittle,&quot; in the sense that if a single component is lost there is a very good chance it will stop functioning properly. Knock out the microprocessor, or the clock, or snip a few wires, and the whole thing will stop running properly.</p>
<p>Neural networks, by contrast, gracefully degrade. If you lose a few neurons and /or synapses, there is a good chance that the whole system will continue to function well. Of course, if you lose enough neurons and synapses it will show, but the damage in performance is generally proportional to the damage to the network. The network degrades in perfromance &quot;gracefully.&quot; Or, as it is also put, it is &quot;fault tolerant.&quot; This is related to the fact that it operates in parllel rather than serial. While a serial computer needs to have every component lined up properly to work, a neural network has lots of redundant wiring which can compensate when issues arise. </p>
<p>It's easy to simulate graceful degradation in Simbrain. Create a network that does something and then start snipping out neurons or synapses. Does it keep working? How drastically is performance effected?</p>
<p class="heading2">Tolerance of Noisy Inputs</p>
<p>Digital computers don't like noisy input: they respond only to clean, precise inputs. Anyone who has worked with computers--that is, any modern homo sapiens--has some understanding of this. To get through the phone tree you have to enter just the right sequence of numbers, no mistakes allowed! Suppose you are looking me up in  a database. If you enter &quot;Joshimi&quot; instead of &quot;Yoshimi&quot; you won't find me. You must enter the exact right input to get the right response. (Of course Google and other computers can make good guesses, but then they are using technologies that may be inspired by neural network ideas; they are approximating neural network like computation in a digital computer).</p>
<p>Neural networks , on the other hand, do quite well with noisy inputs. Just look at our brains. Show me ten roses, and the exact pattern of stimulation on my retina will differ. In fact, show me the same rose ten times, and there is sure to be noise in the pattern produced on my retina. But I see it as a rose every time. We will see lots of examples of neural networks which keep chugging along even though the inputs presented to them are noisy. </p>
<p>Note that this is just graceful degradation applied to inputs rather than processing components. Brains do well with noisy, &quot;degraded&quot; in puts, but digital computers generally don't. </p>
<p class="heading2">Distributed vs. Local Representations</p>
<p>This distinction is not as much a distinction between digital computers and neural networks, but it is related.</p>
<p>Some representations of objects are stored in a single location or set of locations in a system. This is how most memory systems in a conventional compute work. However, the representation of activity in a neural network is often spread across numerous nodes, which often take continuous values.</p>
<p>We can make this distinction even within the context of neural networks. A local representation in this context is one that is &quot;local&quot; to a particular node. Let us say that a local representation in a neural network corresponds to a non-zero level of activation in just one unit, while a distributed representation corresponds to an array of different levels of activation across a whole set of units. </p>
<p>INSERT PICTURE. </p>
<p>Some older types of network use only local representations, and it remains useful to use local representations. </p>
<p>Of course, the problem with local representations is that you lose some of the virtues above, in particular graceful degradation. If there is just one units whose activation represents my grandmother, then if I lose that neuron I lose my whole memory of my grandmother. But that's just not plausible (it is sometimes derisively called the &quot;grandmother cell doctrine.&quot; Our brains aren't that brittle. </p>
<p class="heading2">Neural Networks process Continuous Numerical Data </p>
<p>Classical computers deal with two kinds of data. At a low level, they deal with digital data, 0's and 1's. At the higher level of a programming language, like C or java, they deal with sentences in an artificial language. In classical AI the assumption has been that the mind is like a computer. The basic units of processing are symbols, sentences in a &quot;language of thought.&quot; Thus, for example, it is assumed that in transforming present tense sentences into the past tense, that the brain takes in the present tense symbols, applies a rule, and then produces the newly transformed output. </p>
<p>Neural networks are different in both ways.</p>
<p>First, at a low level neural networks, while they can sometimes use binary units, often deal with continuos valued data (that is, numbers which can be anywhere in some range, e.g. anywhere in the interval from 0 to 1). Of course, neurons are &quot;all or nothing&quot; in that they either fire or don't fire an action potential, but what neural network researchers tend to model is firing <em>rate</em>, which is continuosly valued. </p>
<p>Second, at a higher level, neural networks are not thought of as working with discrete, language like symbols but rather with high dimensional vectors of numerical data. </p>
<p class="heading2">Conclusion</p>
<p>Neural networks operate in parallel on distributed continuous numerical data in such a way that they degrade gracefully and are tolerant of noisy data. Digital computers, by contrast, oeprate in serial on local, discrete, symbolic data in such a way that they degrade brittley and are not tolerant of noisy data. </p>
<p>&nbsp;</p>
<p>TODO: Rules, rules and representations approach. Get some references. Read CCT 5. </p>
</body>
</html>
