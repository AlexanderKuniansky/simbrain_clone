<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Vectors</title>
<link href="Styles.css" rel="stylesheet" type="text/css">

</head>

<body>
<p><span class="heading">Neural Networks as Dynamical Systems</span></p>
<p>One prevalent way of reconstructing the behavior of a neural network is in terms of dynamical systems theory. As a first pass, we can think of a dynamical system as a rule which tells us how a system changes its state over time. In the context of neural networks, a dynamical system will tell us how, given an initial assignment of activations to the neurons of a network (and assuming the weights are fixed), those activations will change over time. In fact, most neural networks just are dynamical systems: they provide us with a way to say how a set of neurons change their state over time. What dynamical systems theory adds is a mathematical and visual way for thinking about this. </p>
<p>Since the presentation in these modules is relatively non-technical, we will develop these ideas in a visual manner. </p>
<p class="heading2">Vector Spaces </p>
<p>Our first step in the direction of visually understanding the behavior of a neural network is to return to the discussion of vectors from a previous module and show how they correspond to points in a &quot;vector space.&quot;</p>
<p>Recall that a vector is a list of numbers, each of which is called a &quot;component,&quot; and that the dimension of a vector corresponds to how many components it has. Every vector of a given dimension can be thought of as a point in a vector space with that same number of dimensions. For example, this is a list of 2-dimensional vectors can be thought of as a set of <strong>points</strong> in a 2-dimensional vector space:</p>
<blockquote>
  <p>(0,0)<br>
(0,1)<br>
(1,0)<br>
(1,1)</p>
</blockquote>
<p>How do we turn these vectors in to point in a space? Well, you should all know how from high school algebra. We associate 2-dimensional vectors with points in a 2-dimensional space with treating each component as a location along one dimension of the space. For example, to find the point corresponding to the vector (5, 9) we move  5 units along the first x dimension and 9 units along the second y dimension to find our point. The examples above give ius the corners of a square in the first quadrant of the plane. Here is another example:</p>
<blockquote>
  <p><img src="images/2dSpace.gif" width="173" height="104"></p>
</blockquote>
<p>The philosopher Rene Descartes is said to have come up with this method while obvserving flies on his ceiling. He observed that the position of the flies on the ceiling could be described with two numbers, in just this way. In fact, vectors and points are so closely related that  in what follows we use the two terms, &quot;point&quot; and &quot;vector&quot; interchangably.</p>
<p>The point can be generalized to vectors and spaces of any dimension. A vector, remember, is just a list of numbers. For 1-dimensional vectors this is easy: we just associate numbers with points on a line. We already saw how to do this for 2-dimensional vectors. We associate 3-dimensional vectors with points in a three-dimensional space (such as the one we live in), e.g. points bounded by a cubical region. In fact we have names for the first two dimensions: the line and the plane. </p>
<p>But what do we do about vectors which are more than four dimension? For example, these 5-dimensional vectors:</p>
<blockquote>
  <p>(0,-1, 1, .4, 9)<br>
(-1, 2, 4, -3, 9)<br>
(0, 0, 0, -1, -1 )<br>
(0, -1, 0, -1, 0 )<br> 
  </p>
</blockquote>
<p>We can't directly visualize a 5-dimensional space. So what is to be done?</p>
<p>With a lot of practice, mathematicians are able to obtain pretty good intuitions about these spaces, even though they can't be visualized. One thing you can do is think of the plane (a 2-d space) as a bunch of lines next to each other, and then 3-space is a bunch of planes next to each other. 4-spaces are then a bunch of 3-spaces next to each other, 5 spaces are a bunch of 4 spaces next to each other, and so forth. For a neat demonstration of this see <a href="http://www.cut-the-knot.org/ctk/Tesseract.shtml">this link.</a></p>
<p>Another piece of information that can be used in thinking about high dimensional spaces is <strong><em>distance</em></strong>. Vector spaces are <strong>metric spaces</strong>, which means that between any two points a number called a &quot;distance&quot; can be computed (subject to a few additional constraints we won't go into here). How this works is pretty intuitive. For example, (1,1) and (1,.9) are close to each other in the plane, and intuitively also. The first component is the same and the second components just differ by .1. On the other hand, (0,5) and (-1000, 91234) are pretty far apart on both dimensions and hence in the plane. The same reasoning applies to high dimensional spaces. ( 0, 0, 0, 0, 1, 0) is close to ( 0, 0, 0, 0, .9, 0) but far away from ( 100, 99, 283, -102, 5, 23). </p>
<p>How do we compute this? There are multiple ways, but most of us are familiar with <strong>Euclidean distance</strong>, at least in practice. To get the distance between two points in 1-d space, we simply take the absolute value of the difference. E.g,. 4 and 2 are | 2 - 4 | = 2 apart. To get the distance between two points (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>) in a 2-dimensional space we take the square root of (x<sub>1</sub> - x<sub>2</sub>)<sup>2</sup> + (y<sub>1</sub> - y<sub>2</sub>)<sup>2</sup>. To get the distance between two points (x<sub>1</sub>, y<sub>1</sub>, z<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>) in a 3-dimensional space we take the square root of  (x<sub>1</sub> - x<sub>2</sub>)<sup>2</sup> + (y<sub>1</sub> - y<sub>2</sub>)<sup>2</sup> + (z<sub>1</sub> - z<sub>2</sub>)<sup>2</sup>. This easily generalizes to arbitrarily many dimensions. (See the <a href="http://en.wikipedia.org/wiki/Proximity">wikipedia entry on distance</a>.) </p>
<p>In addition to allowing us to think intuitively about distances between points, distance information can be used to create <strong>projections</strong> from high dimensional spaces which we can't visualize to two or three dimensional spaces we can visualize. A good projection will preserve as much distance information as possible. We are all familiar with flat projections of the earth, which take a 3-dimensional object (a globe) and show it in a lower dimensional space (a map of the earth). Merced and Fresno are near each other on the 2-d map, and they are near each other on the globe, so this is a good projection. Another, less famililar example, is a &quot;boquet of circles,&quot; which is a figure in six dimensional space, which is set of three circles, each lying on its own 2-d plane, whic intersect at a single point. This object exists in 6 dimensions, but can be projected to dimensions and thereby visualized:</p>
<p><img src="images/S1VS1VS1_Sammon.gif" width="300" height="300"></p>
<p>The software that was used to do the visualization is part of Simbrain (it was created by one of the authors and a mathematician; for more information click <a href="http://hisee.sourceforge.net/">here</a>), and so can be used to visualize structures in the high dimensional activation, weight, and input spaces of a neural network. </p>
<p>How does all this connect to dynamical systems theory? Recall that a dynamical system provides a rule for saying how a system changes state over time. The &quot;states&quot; of a system often correspond to vectors (or &quot;state vectors,&quot;) each of whose components is the state of a &quot;state variable.&quot; For example, the state of a neural network is often considered to be its activation vector. The set of all possible states of a system, e.g. all possible activation vectors for a neural network, is its <strong>state space.</strong> A dynamical system tells us how, from any point in state space (given any state vector for a system, e.g. any specification of activations for the nodes of a neural network), that system will evolve. In other words, for each point in state space, the dynamical system will tell us what points will be visited at all future times. </p>
<p class="heading2">State Space</p>
<p>Now, how does all of this apply to neural networks?</p>
<p>Recall from on earlier module that there are several standard features of neural networks which are understood as vectors: activation vectors, weight vectors or matrices, and input / output vectors. In every case we can think of the relevant sets of vectors as points in a vector space. This in turn allows us to visualize various aspects of neural networks. </p>
<p>Perhaps the most common case is that of activation vectors. The state of a neural network is generally taken to be an activation vector each of whose components corresponds to a state of one of its neurons. Thus, the state of a network with 20 nodes is a point in a 20-dimensional space. These can also, as we'll see in another module, be thought of as representations. </p>
<p>These are called &quot;activation spaces,&quot; and can be thought of as containing all the possible representations of a network. </p>
<p>Let us consider an example. Returning to odor world, here is a very simple network which reacts to objects around it:</p>
<p>INSERT EXAMPLE. TWO OBJECTS + TOTAL SPACE</p>
<p>Now another point about activation vectors is that they can be boken down into parts. Instead of thinking of the above as a vector in 6-space, we can think of it as involving an input vector in 4 space and an output vector in 2-space. Technically, this is known as projecting a point in one space to a point in a corresponding subspace. We can separately plot these as follows. </p>
<p>INSERT SUBSPACE EXAMPLES</p>
<p> Thus we can look at similarity relations between patterns in all these spaces.</p>
<p>In fact, the whoel brain can be thought of as being described by a huge state vector, which can be broken down into sub-vectors. The state of the visual cortex, which represents a visual scene in the world, is a pattern of activity over several million neurons, which can be represented by a million dimensional vector, which is a point in a millions dimensional space. We can now compare different states of the visual cortex by how close or far they are in &quot;visual cortex space.&quot; We can imagine that visual experiences of similar objects in similar lighting conditions are close to each other, while visual experineces of different objects are far away, and if we plotted these as points we would see the relevant distance relations.</p>
<p>Weight vectors are also points in a space with as many dimensions as there are weights (here it is easier to think of sets of weights as list vectors than as matrices, even though matrices are, in the strict mathematical sense, themselves vectors). For example, in the network above there are 16 synapses, and thus we have a point in a 16 dimensional &quot;weight space.&quot; Each point in weights space corresponds to a different set of weights, and hence a different manner of processing inputs and outputs, a different vector-valued function. We can imagine that points nearby will compute similar functions, while points far apart compute different functions. </p>
<p>Inputs are also point in a vector space. This allows us to compare and directly analyze the stimuli to a network. The input space. It is then interesting to see how patterns in this input space are converted into patterns in an intermediate hidden units spaces. We saw these input features above because the input nodes did not process the inputs to them, but rather just presented input values directly. </p>
<p class="heading2">Paths vs. Orbits </p>
<p>Now, as a neural network runs, its state changes. The numbers in its nodes change, and so the activation vector which describe its state changes. This can be visualized as the movement of a point in a high dimensional space, as a sequance of points. For example, in the simple network above, moving the mouse around moves the current state from point to point in the &quot;V-shaped&quot; figure. </p>
<p>If we turn the inputs off, and just set the network in some state and run it, then this is called an &quot;orbit&quot; or &quot;trajectory.&quot; If the external inputs are changing, its' not an orbit (well, the whole system which includes the environmental dynamics is an orbit...) Dynamical systems are closed, so that they can be predictable. </p>
<p>We can now return to dynamcal systems theory. A dynamical system gives us, visually, a set of orbits in state space, which never intersect each other. But we haven't seen any real dynamical systems yet, because we keep letting the inputs change. (Can you think of another way to &quot;break&quot; the dynamics? One way is to add random noise to nodes). </p>
<p>Let us now turn to networks which are dynamical systems. </p>
<p class="heading2">Attractors, Basins of attraction, Limit Cycles </p>
<p>So, suppose you start off in a state, that is, at some point in state space. Now you run the network. The state will change. The activations change as the inputs changes, for example. Or the weights change as the network learns (if the network is not learning, then the weight vectors all stay the same). </p>
<p>But let's focus on a specific case. First, we focus on activation rather than weights. We also focus on cases where the network will change activation dramatically even when the input stays fixed. These are sometimes called &quot;attractor networks&quot; for reasons we'll see.</p>
<p>What kind of network will change its activation  even wit<br>
h fixed inputs? What about the feed-forward networks we've been seeing? They have a tendency to just pass their activation long, like a sieve, sifting out its water. So, what do we do to get interesting behavior? The answer is simple: recurrence.</p>
<p>By adding a recurrent link to a network I mean a weight which creates a loop, so that activation won't just go forward but will loop around. Recurrence is pervasive in the brain.</p>
<p>You might ask how these attractor dynamics change when external inputs are allowed. That is a fascinating topic which is only touched on in the literature, but which I and a colleague have been studying.</p>
<p class="heading2">Philosophy</p>
<p>All of this allows us to expand on the ideas we saw in the module on vectors. </p>
<p>As Paul Churchland put it in his 1986 article in <em>Mind</em>: </p>
<blockquote>
  <p>The basic idea&hellip;&nbsp; is that the brain represents various aspects of reality by a <em>position</em> in a suitable <em>state-space</em>; and the brain performs computations on such representations by means of general coordinate transformations from one state-space to another&hellip; The theory is entirely accessible&mdash;indeed, in its simplest form [under 3 dimensions] it is <em>visually</em> accessible (280).
  </p>
</blockquote>
<p>&nbsp; </p>
</body>
</html>
