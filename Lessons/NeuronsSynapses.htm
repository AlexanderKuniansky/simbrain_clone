<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Basic Function</title>
<link href="Styles.css" rel="stylesheet" type="text/css">
<style type="text/css">
<!--
.style1 {font-weight: bold}
-->
</style>
</head>

<body>
<h2 class="heading"> Basic Function of Neural Networks</h2>
<p>&nbsp;</p>
<p class="heading2">Modeling in general</p>
<p>Computers are useful in studying the brain because the brain is difficult to study directly. Consider. It&rsquo;s relatively easy to study a cabbage. It is easily prodded. Its internal structure is pretty regular. It has no emotions (well, arguably not&mdash;but that is a philosophical question) so we can chop it and cut it at will. And cabbage chunks are easily inspected with a microscope. Now contrast a live brain. It is about the size, in humans, of a small cabbage, yes, but there the differences end. Brains have incredibly complex internal structure. Living brains &ldquo;have&rdquo; or at least produce feelings like pain, so we ought to be careful in our cuttings and proddings. And even if we do manage to stick in an electrode without violating ethical principles, all we get is high-level, summary information about what lots and lots of neurons are doing.</p>
<p>For these reasons, computers help. A computer simulation of a brain feels no pain (again, arguably; that is also a philosophical question and a much hotter one than the one about cabbages), so we can play with it as we wish. And the computer simulation is accessible. We can zoom in and out on simulated neurons, change their simulated firing rates, and see what happens in great detail, without disturbing any live tissue.</p>
<p>It is important to note that models make simplifying assumptions. Thus, any particular model of the brain will leave many things out. This is fine, as long as one is aware of them.&nbsp; In order to get a simulation off the ground certain aspects of the reality in question <em>have to be ignored</em>.&nbsp; The effects of mitochondria, glial cells, and a whole range of cellular machinery are left out of the connectionist equations overviewed in sections following.&nbsp;&nbsp; Forgetting this point can be disastrous.&nbsp; But if kept firmly in mind, simplifying assumptions are harmless and even helpful (cf the maxims of empirical science in 3.3).</p>
<p>Three kinds of analysis</p>
<p>Psychological, biological, computational.</p>
<p>Always ask: what are the inputs and outputs? Everything has to be represented as groups of numbers. So, what are the numbers? </p>
<p>Basics of brain: neurons, synapses, etc.</p>
<p>In this module  we look in more detail at the different components of neural networks, the neurons and the weights, and see how they can do interesting things. </p>
<p>First, we ought to step back for some very basic neuroscience. The brain is composed of about 100-billion neurons, connected by about 100-trillion synpases. In another module we saw how, at a very general level, neural networks capture this with inputs, outputs, and internal processing. In this module we will study how individual neuronns and synapses are modeled, so let us consider those in more detail.</p>
<p class="heading2">Biological Neurons </p>
<p>A real biological neuron is a cell (a &quot;nerve cell&quot;) like any other, which is specialized for transmitting electrical information in the brain. It has all the complex machinery any cell has, including mitochondria, golgi apparatus, a nucleus, and a complex membrane with tons of fancy proteins embedded in it. Of all this stuff what neural networks model is the way they take in information and pass it along. They do this via a few specialized structures:</p>
<blockquote>
  <p><strong>Dendrites</strong>: these are extensions that grow out of the cell body like a big root complex (dendrite in Latin means tree). You can think of them, for these purposes, as a giant catcher's mitt, because it is here that signals from other neurons are received, in the form of influxes of electrical charge, which affect the voltage of the neuron. </p>
  <p><strong>Soma</strong> or cell body: it is here that all the information from other neurons is summed together. If the voltage at the soma is sufficient, the neuron will fire an action potential along its axon. </p>
  <p><strong>Axon</strong>: this is another extension growing out of the cell body. They can be different lengths, but they are often longer than dendrites, having a long main extension and terminating in a branched structure. When the neuron fires an action potential, electrical activity propagates down these extensions, and stimulates dendrites of other neurons, and the process continues. The axon is like the arm of a pitcher, throwing signals to other dendrites which catch them. </p>
  <p> <strong>Synapses</strong>: these are not a part of neurons per se, but are rather the tiny junctions between neurons, where an axon touches (or almost touches) a dendrite. When an action potential reaches a synapse, chemicals are released into it, which open up channels on the dendrite of the connected neuron, which lets charge into it, and the process described above continues. Synapses can be stronger or weaker insofar as they pass more or less charge along. </p>
</blockquote>
<p>These four structures and the aspects of them mentioned above correspond to the main features of the brain modelled by neural networks. Notice how many simplifying assumptions are being made: we are not looking at the temperature of the neurons, any of the organelles, etc. </p>
<p>Here is a picture which shows these  components:</p>
<p align="center"><img src="images/neuron_large.gif" width="257" height="344"> </p>
<p>The process begins at the top of this diagram, where a dendrite catches incoming signals. They are summed at the soma, and the signal is passed along via synapse, shown in the inset. </p>
<p class="heading2">Artificial Neurons and Synapse</p>
<p>One can go pretty far in modeling neurons and synapses, and in another module we will see more details about how this is done. But neural networks usually begin with a simple base-line type of model   which we describe here. These neuron-like elements (here &quot;node&quot; and &quot;unit&quot; are most appropriate) capture the basic information processes described above. Similarly with the synapse like elements, which we call &quot;weights.&quot; </p>
<p>We need to start with some basic quantities, and ask what they represent. </p>
<blockquote>
  <p><strong>Node Activation</strong>. Artifical neurons are associated with a quantity called &quot;activation,&quot; which we will denote <em>a</em>. Sometime we will have a set of neurons in a group, and then we will attach an index <em>i</em> to these neurons, as in <em>a</em>1, <em>a</em>2, <em>... </em>an.</p>
  <blockquote>
    <p><br>
      What this models.
    The activation of a node corresponds to a degree of activity. It can roughly be thought of as representing at what rate a neuron is firing. Or, what its voltage potential is (this later). But we can just think of it as how active a neuron is. Sometimes units aren't even thought of as representing individual neurons, in which case there is no biological correspondence at all. </p>
    <p>In Simbrain this is the number that shows up, and corresponds to a color. </p>
  </blockquote>
  <p><strong>Synapse Strength</strong>. Artificial synpases or weights are also associated with numbers, which we can think of as their &quot;strength.&quot; We denote this <em>w</em>. The index is more complicated though, since weights are a kind of connection, which links one neuron to another. Thus we want to know what the source or &quot;pre-synaptic neuron&quot; is, and what the target or &quot;post-synaptic neuron&quot; is. We will use two indices, first the source neuron, the second for the target, as in <em>w</em>12 which is a weight which connects node 1 with node 2. Node 1 is pre-synaptic neuron, node 2 is the post-synaptic neuron. In real synapses it is not yet known exactly what strength corresponds to. </p>
  <blockquote>
    <p>What this models: People just now that when the measure pre and post synaptically that synapse changes in how much affect it has. but there are some ideas. It may be the number of receptors post-synaptically, or the amount of vesicles released, or the number of actual spines. Of ourse in many models, again, these don't represent neurons so they are just an abstract representation of coupling strength between units. </p>
    <p> Simbrain: In Simbrain this is a color and a size. </p>
  </blockquote>
  <p><strong>Inputs</strong>: These come from external sources. We call these couplings in Simbrain. </p>
  <p><strong>Weighted inputs. </strong>Ok, now we need one more notation, a fundamental quantity: net_input or weighted_input. We will denote this by a capital <strong>W</strong>. This represented a weighted sum of all the signals coming in to a neuron from other neurons, together with any external inputs if it's linked to the environment, which we call <em>I. </em> To calculate W we use:</p>
  <blockquote>
    <p><img src="images/WeightedSum.gif" width="187" height="78"> </p>
  </blockquote>
  <p>That is, the weighted inputs to a neuron <em>j</em> is the sume of the activations of pre-synaptic neurons <em>a</em><sub>i </sub> times weights <em>w</em><sub>ij</sub> plus any external inputs <em>I. </em>For example:</p>
  <p>&nbsp; </p>
</blockquote>
<p>So we have:</p>
<blockquote>
  <p><span class="style1"><em> a</em><sub>i</sub></span>: activation of neuron <em>i</em>.</p>
  <p><strong>w<sub>ij</sub></strong>: strength of connection from neuron <em>i</em> to neuron <em>j</em>.<em>I</em>: external inputs to a neuron.</p>
  <p><strong>W<sub>j</sub></strong>: weighted inputs to neuron <em>j</em>.</p>
</blockquote>
<p>&nbsp;</p>
<p class="heading2">Activation functions</p>
<p>Now that we've defined all these quantities, we are  in a position to define some basic activation functions. These are rules which update activations <em>a</em> of neurons as a function of weighted inputs W (though we will se other rules in other modules which don't use W). </p>
<p>We begin with a very basic activation function, which goes back to one of the earliest discussions of neural networks in McCullock and Pitts.</p>
<blockquote>
  <p>Binary Function</p>
  <p><img src="images/BinaryGraph.gif" width="161" height="110"></p>
  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p><img src="images/LinearGraph.gif" width="164" height="104"></p>
  <p><br>
    - Binary</p>
</blockquote>
<p>&nbsp;</p>
<p>- Sigmoidal</p>
<p>&nbsp;</p>
<p><img src="images/SigmoidalGraph.gif" width="161" height="107"></p>
<p><br>
  Other things: noise, upper bounds, bias. <br>
</p>
<p class="heading2">Biological Synapses</p>
<p>Artificial Synapses or Weights . We will see weights later. </p>
<p>The Environment</p>
<p> Example 1: Activation Functions</p>
<p>Example 2: Retina</p>
<p>Example 3: Hot and Cold</p>
<p>&nbsp;</p>
</body>
</html>
